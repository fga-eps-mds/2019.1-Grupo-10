<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>Documento de estudo da US08 - Pylearner</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  <link href="../../css/extra.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Documento de estudo da US08";
    var mkdocs_page_input_path = "documentation-mds/Documento-da-US08.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Pylearner</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Políticas</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../policies/Branches/">Branches</a>
                </li>
                <li class="">
                    
    <a class="" href="../../policies/Commits/">Commits</a>
                </li>
                <li class="">
                    
    <a class="" href="../../policies/Definicao-Done/">Definicao Done</a>
                </li>
                <li class="">
                    
    <a class="" href="../../policies/Git-Workflow/">Git Workflow</a>
                </li>
                <li class="">
                    
    <a class="" href="../../policies/Style-Guide/">Style Guide</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Documentação</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../Documento-de-Arquitetura/">Documento de Arquitetura</a>
                </li>
                <li class="">
                    
    <a class="" href="../Documento-de-Visao/">Documento de Visão</a>
                </li>
                <li class="">
                    
    <a class="" href="../Documento-de-Metodologia/">Documento de Metodologia</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Tap/">TAP</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Eap/">EAP</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Backlog/">Backlog</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Roadmaps</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../roadmap/Roadmap-TechLead/">Roadmap TechLead</a>
                </li>
                <li class="">
                    
    <a class="" href="../../roadmap/Roadmap-DevOps/">Roadmap DevOps</a>
                </li>
                <li class="">
                    
    <a class="" href="../../roadmap/Roadmap-Arquiteto/">Roadmap Arquiteto</a>
                </li>
                <li class="">
                    
    <a class="" href="../../roadmap/Roadmap-Projeto/">Roadmap Projeto</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Documentação das sprints</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../sprints/Sprint-0-review/">Sprint 0 review</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sprints/Sprint-1-planning/">Sprint 1 planning</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sprints/Sprint-1-Review/">Sprint 1 Review</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sprints/Sprint-2-planning/">Sprint 2 planning</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sprints/Sprint-2-review/">Sprint 2 review</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sprints/Sprint-3-planning/">Sprint 3 planning</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sprints/Sprint-3-review/">Sprint 3 review</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sprints/Sprint-4-planning/">Sprint 4 planning</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sprints/Sprint-4-review/">Sprint 4 review</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sprints/Sprint-5-planning/">Sprint 5 planning</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sprints/Sprint-5-review/">Sprint 5 review</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sprints/Sprint-6-planning/">Sprint 6 planning</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Documentação do Estudo</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../Documento-da-US01/">Documento de estudo da US1</a>
                </li>
                <li class="">
                    
    <a class="" href="../Documento-da-US02/">Documento de estudo da US02</a>
                </li>
                <li class="">
                    
    <a class="" href="../Documento-da-US03/">Documento de Estudo da US03</a>
                </li>
                <li class="">
                    
    <a class="" href="../Documento-da-US04/">Documento de estudo da US04</a>
                </li>
                <li class="">
                    
    <a class="" href="../Documento-da-US05/">Documento de Estudo da US05</a>
                </li>
                <li class="">
                    
    <a class="" href="../Documento-da-US14/">Documento de Estudo da US14</a>
                </li>
                <li class="">
                    
    <a class="" href="../Documento-da-US15/">Documento de estudo da US15</a>
                </li>
                <li class="">
                    
    <a class="" href="../Documento-da-US16/">Documento de Estudo da US16</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Pylearner</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
    
    <li>Documento de estudo da US08</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="documento-de-estudo-da-us08">Documento de estudo da US08</h1>
<h2 id="introducao">Introdução</h2>
<p>Antes da criação das intents, actions e stories de cada tema selecionado são gerados documentos pela equipe de desenvolvimento que possuem abrangentemente do que aquele ele se trata. Selecionamos um breve conhecimento do tema, possíveis bibliotecas e até mesmo a forma de implementação. </p>
<h2 id="objetivo">Objetivo</h2>
<p>Tendo em vista que os dados passados para o treinamento do bot devem ser bem claros e ter uma quantidade reduzida de informação para não poluir a caixa de mensagens do usuário, o desenvolvedor responsável pelo tema tem como objetivo extrair o máximo de informação e simplifica-la antes de passar para o usuário final. Isso torna a criação das intents, actions e stories mais rápidas e objetivas.</p>
<h2 id="1-generalized-linear-models">1. Generalized Linear Models</h2>
<p>Em estatística, o modelo linear generalizado é uma generalização flexível da regressão linear ordinária que permite variáveis de resposta que possuem modelos de distribuição de erro diferentes de uma distribuição normal. O GLM generaliza a regressão linear permitindo que o modelo linear seja relacionado à variável de resposta por meio de uma função de link e permitindo que a magnitude da variância de cada medida seja uma função de seu valor previsto.</p>
<h3 id="11-logistic-regression">1.1 Logistic regression</h3>
<p>A regressão logística, apesar de seu nome, é um modelo linear para classificação em vez de regressão. A regressão logística também é conhecida na literatura como regressão logit, classificação de máxima entropia (MaxEnt) ou o classificador log-linear. Nesse modelo, as probabilidades que descrevem os resultados possíveis de um único teste são modeladas usando uma função logística.</p>
<pre><code class="python">from sklearn import svm
X = [[0, 0], [1, 1]]
y = [0, 1]
clf = svm.SVC(gamma='scale')
clf.fit(X, y)

Output:

SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
    tol=0.001, verbose=False)

</code></pre>

<h2 id="2-support-vector-machines">2. Support Vector Machines</h2>
<p>A máquina de vetores de suporte (SVM) é um conjunto de métodos do aprendizado supervisionado que analisam os dados e reconhecem padrões, usado para classificação e análise de regressão. O SVM padrão toma como entrada um conjunto de dados e prediz, para cada entrada dada, qual de duas possíveis classes a entrada faz parte, o que faz do SVM um classificador linear binário não probabilístico. Um modelo SVM é uma representação de exemplos como pontos no espaço, mapeados de maneira que os exemplos de cada categoria sejam divididos por um espaço claro que seja tão amplo quanto possível.</p>
<h3 id="21-classificacao">2.1 Classificação</h3>
<p>SVC, NuSVC e LinearSVC são classes capazes de executar classificações de várias classes em um conjunto de dados.</p>
<p>O SVC e o NuSVC são métodos semelhantes, mas aceitam conjuntos de parâmetros ligeiramente diferentes e têm diferentes formulações matemáticas. Por outro lado, o LinearSVC é outra implementação do Support Vector Classification para o caso de um kernel linear.</p>
<p>Como outros classificadores, SVC, NuSVC e LinearSVC tomam como entrada duas matrizes: uma matriz X de tamanho [n_samples, n_features] contendo as amostras de treinamento, e uma matriz y de rótulos de classe (strings ou inteiros), tamanho [n_samples]:</p>
<p>Depois de ajustado, o modelo pode ser usado para prever novos valores:</p>
<pre><code class="python">clf.predict([[2., 2.]])

Output:

array([1])
</code></pre>

<p>A função de decisão de SVMs depende de algum subconjunto dos dados de treinamento, chamados de vetores de suporte. Algumas propriedades destes vetores de suporte podem ser encontradas nos membros support_vectors_, support_ e n_support:</p>
<pre><code class="python"># get support vectors
clf.support_vectors_

Output:

array([[0., 0.],
       [1., 1.]])

</code></pre>

<pre><code class="python"># get indices of support vectors
clf.support_

Output:

array([0, 1]...)
</code></pre>

<pre><code class="python"># get number of support vectors for each class
clf.n_support_

Output:

array([1, 1]...)

</code></pre>

<h3 id="211-multi-class-classification">2.1.1 Multi-class classification</h3>
<p>O SVC e o NuSVC implementam a abordagem “um-contra-um” para classificação de várias classes. Se n_class é o número de classes, os classificadores n_class * (n_class - 1) / 2 são construídos e cada um treina os dados de duas classes. Para fornecer uma interface consistente com outros classificadores, a opção decision_function_shape permite agregar os resultados dos classificadores “um contra um” a uma função de decisão de forma (n_samples, n_classes):</p>
<pre><code class="python">X = [[0], [1], [2], [3]]
Y = [0, 1, 2, 3]
clf = svm.SVC(gamma='scale', decision_function_shape='ovo')
clf.fit(X, Y)

Output:

SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
    decision_function_shape='ovo', degree=3, gamma='scale', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
    tol=0.001, verbose=False)

</code></pre>

<pre><code class="python">dec = clf.decision_function([[1]])
dec.shape[1] # 4 classes: 4*3/2 = 6

Output:
6
</code></pre>

<pre><code class="python">clf.decision_function_shape = &quot;ovr&quot;
dec = clf.decision_function([[1]])
dec.shape[1] # 4 classes

Output:
4
</code></pre>

<p>Por outro lado, o LinearSVC implementa uma estratégia multi-classe “one to the rest”, treinando assim modelos n_class. Se houver apenas duas classes, apenas um modelo é treinado:</p>
<pre><code class="python">lin_clf = svm.LinearSVC()
lin_clf.fit(X, Y)

Output:
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)
</code></pre>

<pre><code class="python">dec = lin_clf.decision_function([[1]])
dec.shape[1]

Output:
4
</code></pre>

<h2 id="3-stochastic-gradient-descent">3. Stochastic Gradient Descent</h2>
<p>O Descentramento Estocástico de Gradiente (SGD) é uma abordagem simples, mas muito eficiente, para a aprendizagem discriminativa de classificadores lineares sob funções de perda convexa, como Máquinas de Vetores de Suporte (lineares) e Regressão Logística. O SGD tem sido aplicado com sucesso em problemas de aprendizado de máquina em grande escala e esparsos, freqüentemente encontrados na classificação de textos e processamento de linguagem natural.</p>
<h3 id="31-classification">3.1 Classification</h3>
<p>A classe SGDClassifier implementa uma rotina de aprendizado de descida gradiente estocástica que suporta diferentes funções de perda e penalidades para classificação.</p>
<p>Como outros classificadores, o SGD deve ser equipado com duas matrizes: uma matriz X de tamanho [n_samples, n_features] contendo as amostras de treinamento e uma matriz Y de tamanho [n_samples] contendo os valores de destino (rótulos de classe) para as amostras de treinamento:</p>
<pre><code class="python">from sklearn.linear_model import SGDClassifier
X = [[0., 0.], [1., 1.]]
y = [0, 1]
clf = SGDClassifier(loss=&quot;hinge&quot;, penalty=&quot;l2&quot;, max_iter=5)
clf.fit(X, y)

Output:
SGDClassifier(alpha=0.0001, average=False, class_weight=None,
           early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,
           l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,
           n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',
           power_t=0.5, random_state=None, shuffle=True, tol=None,
           validation_fraction=0.1, verbose=0, warm_start=False)
</code></pre>

<p>Depois de ajustado, o modelo pode ser usado para prever novos valores:</p>
<pre><code class="python">clf.predict([[2., 2.]])

Output:
array([1])
</code></pre>

<p>O SGD ajusta um modelo linear aos dados de treinamento. O membro coef_ contém os parâmetros do modelo:</p>
<pre><code class="python">clf.coef_                                      

Output:
array([[9.9..., 9.9...]])
</code></pre>

<p>Membro intercept_ detém a interceptação (deslocamento ou polarização):</p>
<pre><code class="python">clf.intercept_                                 

Output:
array([-9.9...])
</code></pre>

<p>Se o modelo deve ou não usar uma interceptação, ou seja, um hiperplano enviesado, é controlado pelo parâmetro fit_intercept.</p>
<p>Para obter a distância assinada para o hiperplano use SGDClassifier.decision_function:</p>
<pre><code class="python">clf.decision_function([[2., 2.]])              

Output:
array([29.6...])
</code></pre>

<p>A função de perda de concreto pode ser definida através do parâmetro de perda. O SGDClassifier suporta as seguintes funções de perda:</p>
<blockquote>
<p>loss = "hinge": (soft-margin) linear Máquina de Vetores de Suporte</p>
<p>loss = "modified_huber": perda da dobradiça alisada</p>
<p>loss = "log": regressão logística</p>
<p>E todas as perdas de regressão abaixo</p>
</blockquote>
<p>Usando loss = "log" ou loss = "modified_huber" habilita o método predict_proba, que fornece um vetor de estimativas de probabilidade por amostra:</p>
<pre><code class="python">clf = SGDClassifier(loss=&quot;log&quot;, max_iter=5).fit(X, y)
clf.predict_proba([[1., 1.]])                  

Output:
array([[0.00..., 0.99...]])
</code></pre>

<h2 id="4-nearest-neighbors">4. Nearest Neighbors</h2>
<p>Nearest Neighbors e a base de muitos outros métodos de aprendizado. O pricípio por trás do método nearest neighbors é encontrar um número predefinido de amostras de treinamento mais próximas da distancia do novo ponto e prever a label a partir delas. O número de amostras pode ser uma constante definida pelo usuário (k-neighbor learning) ou variar com base na densidade local dos pontos (radius-based neighbor learning). A distancia pode ser qualquer medida métrica. Sendo um método não paramêtrico, é bem sucedido em situações de classificação em que os limites de decisão são muito irregulares.</p>
<h3 id="41-unsurpevised-nearest-neighbors">4.1 Unsurpevised nearest neighbors</h3>
<p><a href=https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors>NearestNeighbors</a> atua como uma interface uniforme para três tipos de algoritmos nearest neighbors: <a href=https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree>BallTree</a>, <a href=https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree>KDTree</a>, e um algoritmo de força bruta baseado nas rotinas da <a href =https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics.pairwise><code>sklearn.metrics.pairwise</code></a>. A escolha dos algoritmos é controlada através do parâmetro <code>algorithm</code>, que deve ser <code>['auto', 'ball_tree', 'kd_tree', 'brute']</code>. Quando o valor default <code>'auto'</code> e passado, o algoritmo tenta determinar a melhor abordagem a partir dos dados de treinamento.</p>
<h4 id="411-achando-o-nearest-neighbors">4.1.1 Achando o Nearest Neighbors</h4>
<p>Exemplo de como achar nearest neighbors entre dois conjuntos de dados:</p>
<pre><code class="python">from sklearn.neighbors import NearestNeighbors
import numpy as np

X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])

nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)
distances, indices = nbrs.kneighbors(X)

Out:
indices array([[0, 1],
               [1, 0],
               [2, 1],
               [3, 4],
               [4, 3],
               [5, 4]]...)

distances array([[0.        , 1.        ],
                 [0.        , 1.        ],
                 [0.        , 1.41421356],
                 [0.        , 1.        ],
                 [0.        , 1.        ],
                 [0.        , 1.41421356]])

</code></pre>

<h4 id="412-classes-kdtree-e-balltree">4.1.2 Classes KDTree e BallTree</h4>
<p>Outra alternativa é usar diretamente as classes KDTree e BallTree para achar os nearest neighbors. As duas classes tem a mesma interface:</p>
<pre><code class="python">from sklearn.neighbors import KDTree
import numpy as np
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
kdt = KDTree(X, leaf_size=30, metric='euclidean')
kdt.query(X, k=2, return_distance=False)          
array([[0, 1],
       [1, 0],
       [2, 1],
       [3, 4],
       [4, 3],
       [5, 4]]...)
</code></pre>

<h3 id="42-classisficacao-nearest-neighbors">4.2 Classisficação Nearest Neighbors</h3>
<p>A classificação usando nearest neighbors é calculada a partir de uma simples votação por maioria dos nearest neighbors mais próximos de cada ponto: um ponto de consulta é atribuido à classe de dados que possui mais representantes dentro dos nearest neighbors mais próximos.</p>
<p>O scikit-learn implementa dois classificadores de nearest neighbors: <a href=https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier>KNeighborsClassifier</a> implementa o aprendizado baseado no <em>k</em> nearest neighbors de cada ponto de consulta,onde <em>k</em> é um valor inteiro especificado pelo usuário. <a href=https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier>RadiusNeighborsClassifier</a> implementa a aprendizagem com base no número de neighbors dentro de um raio fixo <em>r</em> de cada ponto de treinamento, onde <em>r</em> é um valor de ponto flutuante especificado pelo usuário.</p>
<p>A classificação de vizinhos KNeighborsClassifier é a técnica mais usada. A escolha ideal do valor <em>k</em> é altamente dependente dos dados: em geral, um <em>k</em> muito grande suprime os efeitos do ruído, mas torna os limites de classificação menos distintos.</p>
<p>Nos casos em que os dados não são uniformemente amostrados, a classificação dos neighbors baseados no raio RadiusNeighborsClassifierpode ser uma escolha melhor. O usuário especifica um raio fixo <em>r</em>, de modo que pontos em vizinhanças mais esparsas usem menos vizinhos mais próximos para a classificação.</p>
<p>A classificação básica de vizinhos mais próximos usa pesos uniformes: isto é, o valor atribuído a um ponto de consulta é calculado a partir de uma votação por maioria simples dos vizinhos mais próximos. Em algumas circunstâncias, é melhor ponderar os vizinhos de tal forma que os vizinhos mais próximos contribuam mais para o ajuste. Isso pode ser feito através do parametro <code>weights</code>. O valor padrão <code>weights = 'uniform'</code>, atribui pesos uniformes a cada vizinho.<code>weights = 'distance'</code> atribui pesos proporcionais ao inverso da distância do ponto de consulta. Alternativamente, uma função definida pelo usuário da distância pode ser fornecida para calcular os pesos.</p>
<p>Exemplo:</p>
<p>```python</p>
<p>import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn import neighbors, datasets</p>
<p>n_neighbors = 15</p>
<h1 id="import-some-data-to-play-with">import some data to play with</h1>
<p>iris = datasets.load_iris()</p>
<h1 id="we-only-take-the-first-two-features-we-could-avoid-this-ugly">we only take the first two features. We could avoid this ugly</h1>
<h1 id="slicing-by-using-a-two-dim-dataset">slicing by using a two-dim dataset</h1>
<p>X = iris.data[:, :2]
y = iris.target</p>
<p>h = .02  # step size in the mesh</p>
<h1 id="create-color-maps">Create color maps</h1>
<p>cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])</p>
<p>for weights in ['uniform', 'distance']:
    # we create an instance of Neighbours Classifier and fit the data.
    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)
    clf.fit(X, y)</p>
<pre><code># Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, x_max]x[y_min, y_max].
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

# Put the result into a color plot
Z = Z.reshape(xx.shape)
plt.figure()
plt.pcolormesh(xx, yy, Z, cmap=cmap_light)

# Plot also the training points
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,
            edgecolor='k', s=20)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("3-Class classification (k = %i, weights = '%s')"
          % (n_neighbors, weights))
</code></pre>
<p>plt.show()
 ```</p>
<p>## 5. Naive Bayes</p>
<p><a href=https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB>GaussianNB</a> implementa o algoritmo Gaussian Naive Bayes para classificação. A probabilidade das features assume ser gaussiana:</p>
<p><img alt="" src="img/gaussin-naive-bayes.png" /></p>
<p>Os parâmetros &sigma;<sub>y</sub> e &mu;<sub>y</sub> são estimados usando a maior probabilidade.</p>
<pre><code class="python">from sklearn import datasets
iris = datasets.load_iris()
from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
y_pred = gnb.fit(iris.data, iris.target).predict(iris.data)
print(&quot;Number of mislabeled points out of a total %d points : %d&quot;
      % (iris.data.shape[0],(iris.target != y_pred).sum()))
</code></pre>

<h2 id="6-decision-trees">6. Decision Trees</h2>
<p><a href=https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier>DecisionTreeClassifier</a> e a classe capaz de realizar classificações de varias classes em um dataset.</p>
<p>O parâmetro de entrada são dois arrays: um array X, esparso ou denso, de tamanho <code>[n_samples, n_features]</code> contendo os dados de treino; e um array Y de valores inteiros de tamanho <code>[n_samples]</code>, contendo as labels de classes para os dados de treino:</p>
<pre><code class="python">from sklearn import tree
X = [[0, 0], [1, 1]]
Y = [0, 1]
clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)
</code></pre>

<p>Depois de ajustada, a model pode ser usada para prever os dados:</p>
<pre><code class="python">clf.predict([[2., 2.]])

Saida:
array([1])
</code></pre>
              
            </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../search/main.js" defer></script>

</body>
</html>
